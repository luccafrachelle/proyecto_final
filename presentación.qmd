---
title: "Entrega Final"
author: "Lucca Frachelle , Cecilia Waksman"
date: today
fig-align: center
warning: false
message: false
echo: false
embed-resources: true
mainfont: Arial
sansfont: Arial
background-transition: fade
format:
  revealjs: 
    theme: white
    slide-number: true
    show-slide-number: print
fontsize: 20pt

---

```{python}
#| label: setup
#| output: false
# Importar todas las librerías necesarias
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import pandas as pd
from scipy.stats import norm
from sklearn.manifold import TSNE
from sklearn.metrics.pairwise import euclidean_distances
from sklearn.metrics import silhouette_score
from scipy.stats import spearmanr
from IPython.display import display, HTML
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import pandas as pd
from sklearn import datasets
from sklearn.manifold import TSNE
from sklearn.metrics.pairwise import euclidean_distances
from sklearn.metrics import silhouette_score
from scipy.stats import spearmanr
from IPython.display import display, HTML
import numpy as np
import matplotlib.pyplot as plt
from umap import UMAP
from sklearn.metrics.pairwise import euclidean_distances
from sklearn.metrics import silhouette_score
from scipy.stats import spearmanr
import warnings

# Configuración de estilo consistente para todos los gráficos
plt.style.use('./src/custom_style.mplstyle')
sns.set_palette("husl")

DIGIT_COLORS = ['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728', '#9467bd', 
                '#8c564b', '#e377c2', '#7f7f7f', '#bcbd22', '#17becf']
```

# Introducción

A medida que el volumen y la complejidad de los datos han crecido exponencialmente en la era digital, la **reducción de dimensionalidad** se ha convertido en una técnica indispensable. Su objetivo es transformar datos de alta dimensión en una representación de menor dimensión, preservando al máximo la información relevante y la estructura inherente de los datos. Esta tarea es crucial no solo para la visualización y la comprensión de patrones, sino también para mejorar la eficiencia computacional de algoritmos de aprendizaje automático posteriores y para mitigar la "maldición de la dimensionalidad".

## Enfoques para la Reducción de Dimensionalidad

Existen dos enfoques principales para la reducción de dimensionalidad:

1.  **Factorización de Matrices:**
    -   **PCA (Análisis de Componentes Principales):** Descompone la matriz de datos en componentes principales
    -   **SVD (Descomposición en Valores Singulares):** Factoriza la matriz en matrices ortogonales
    -   **NMF (Non-negative Matrix Factorization):** Factorización con restricción de no negatividad
    -   Características:
        -   Basado en álgebra lineal
        -   Optimiza criterios globales (varianza, error de reconstrucción)
        -   Computacionalmente eficiente
        -   Menos sensible a ruido

## Enfoques para la Reducción de Dimensionalidad
2.  **Grafos de Vecindad:**
    -   **t-SNE:** Construye un grafo de similitud basado en probabilidades
    -   **UMAP:** Construye un grafo de vecindad difuso basado en topología
    -   **LLE (Local Linear Embedding):** Preserva relaciones de vecindad local
    -   Características:
        -   Basado en teoría de grafos y topología
        -   Preserva estructura local de los datos
        -   Mejor para visualización y descubrimiento de clusters
        -   Puede ser computacionalmente más costoso

# Idea general de graph-based dimensionality reduction
La idea general de graph-based dimensionality reduction es la siguiente:

1.  Se construye un métrica de similitud entre los datos en el espacio de alta dimensión.
2. Se busca reecontruir los datos en un espacio de baja dimensión , respetando la similitud entre los datos.
Esto se hace mediante un algoritmo iteativo que busca optimizar la distancia entre los datos en el espacio de baja dimensión para que coincida con la métrica de similitud en el espacio de alta dimensión.

# Toolkit para t-sne
## Softmax:
La fórmula de Softmax para un vector $z = [z_1, z_2, \dots, z_K]$ es:

$$\text{Softmax}(z_i) = \frac{e^{z_i}}{\sum_{j=1}^{K} e^{z_j}}$$

donde:

-   $z_i$ es el $i$-ésimo elemento del vector de entrada.

-   $K$ es el número de elementos en el vector de entrada.

La idea de esta función es normalizar los valores de los elementos de un vector de entrada para que sumen 1. En t-sne, esta función se utiliza para convertir las distancias euclidianas en probabilidades de pertenencia a un cluster.

## La Divergencia Kullback-Leibler (KL)
La fórmula de la KL-Divergencia de una distribución $Q$ con respecto a una distribución $P$ (es decir, $P$ es la "verdadera" o de referencia, y $Q$ es nuestra aproximación) es:

$$\text{KL}(P || Q) = \sum_i P(i) \log \left( \frac{P(i)}{Q(i)} \right)$$

donde:

-   $P(i)$ es la probabilidad de que el evento $i$ ocurra en la distribución $P$.

-   $Q(i)$ es la probabilidad de que el evento $i$ ocurra en la distribución $Q$.

Para t-sne, la KL-Divergencia se utiliza como la función de costo que el algoritmo intenta minimizar. Mide cuán diferentes son las probabilidades de similitud entre puntos en el espacio de alta dimensión ($P$) y en el espacio de baja dimensión ($Q$). El objetivo es hacer que $Q$ sea lo más parecido posible a $P$.

# t-sne
## sne

**Probabilidades en el espacio de alta dimensión:** Para cada par de puntos $x_i$ y $x_j$ en el espacio de alta dimensión, se calcula la probabilidad condicional $p_{j|i}$ de que $x_i$ elija a $x_j$ como su vecino:
    $$p_{j|i} = \frac{\exp(-\|x_i - x_j\|^2 / 2\sigma_i^2)}{\sum_{k \neq i} \exp(-\|x_i - x_k\|^2 / 2\sigma_i^2)}$$

   donde $\sigma_i$ es la varianza de la distribución gaussiana centrada en $x_i$. Esta varianza se ajusta para cada punto $i$ de manera que la distribución de probabilidades $P_i$ tenga una perplejidad fija.

## sne

**Probabilidades en el espacio de baja dimensión:** De manera similar, para los puntos $y_i$ y $y_j$ en el espacio de baja dimensión:

$$q_{j|i} = \frac{\exp(-\|y_i - y_j\|^2)}{\sum_{k \neq i} \exp(-\|y_i - y_k\|^2)}$$

Aquí se usa una distribución gaussiana con varianza fija (1/2) para simplificar.

## sne
**Función de costo:** SNE minimiza la suma de las divergencias KL entre las distribuciones $P_i$ y $Q_i$:

$$C = \sum_i \text{KL}(P_i || Q_i) = \sum_i \sum_j p_{j|i} \log \frac{p_{j|i}}{q_{j|i}}$$
La función de costo es lo que el algoritmo intenta minimizar, con la idea de que las probabilidades en el espacio de baja dimensionalidad sean similares a las del espacio de alta dimensionalidad.



## problemas de sne

1.  **Asimetría en las probabilidades:**
    -   Las probabilidades condicionales $p_{j|i}$ y $p_{i|j}$ no son iguales
    -   Esto puede llevar a resultados inconsistentes
    -   La función de costo es asimétrica respecto a $P$ y $Q$
2.  **El problema de hacinamiento (crowding problem):**
    -   En el espacio de baja dimensión, hay menos "espacio" disponible para que los puntos se separen y las probabilidades de vecindad sean similares a las del espacio de alta dimensionalidad.
    -   Los puntos tienden a aglomerarse en el centro
    -   La distribución gaussiana en baja dimensión no maneja bien este problema
    -   Las colas de la gaussiana decaen muy rápido ($e^{-x^2}$)

## t-sne entra al juego

t-SNE resuelve estos problemas mediante :

1.  **Probabilidades conjuntas simétricas:**
    -   Reemplaza las probabilidades condicionales por probabilidades conjuntas
    -   $p_{ij} = \frac{p_{j|i} + p_{i|j}}{2N}$
    -   Esto asegura que $p_{ij} = p_{ji}$
    -   La función de costo se vuelve simétrica
2.  **Distribución t de Student en baja dimensión:**
    -   Reemplaza la distribución gaussiana por una t de Student con un grado de libertad
    -   La fórmula para $q_{ij}$ se convierte en: $$q_{ij} = \frac{(1 + \|y_i - y_j\|^2)^{-1}}{\sum_{k \neq l} (1 + \|y_k - y_l\|^2)^{-1}}$$

## t-sne entra al juego

```{python}
import numpy as np
import matplotlib.pyplot as plt
import scipy.stats as stats


x = np.linspace(-10, 10, 100)
plt.plot(x, stats.t.pdf(x, 1), label='t-Student')
plt.plot(x, stats.norm.pdf(x), label='Normal')
plt.legend()
plt.show()
```

La distribución t de Student tiene "colas pesadas" (heavy tails), lo que significa que:
 - Asigna una probabilidad significativa a valores lejos de la media - Permite que puntos moderadamente separados en alta dimensión se mapeen a distancias mayores en baja dimensión - Crea más "espacio" en el centro del mapa 
 - Evita la aglomeración excesiva de puntos


Esta función tiene colas que decaen como $1/x^2$, lo que es más lento que la distribución gaussiana que decae como $e^{-x^2}$. Esta propiedad es clave para resolver el problema de hacinamiento.

## la nueva función de costo

La función de costo de t-SNE es una divergencia KL entre las probabilidades conjuntas $P$ y $Q$:

$$C = \text{KL}(P || Q) = \sum_i \sum_j p_{ij} \log \frac{p_{ij}}{q_{ij}}$$


El gradiente de la función de costo con respecto a $y_i$ es:

$$\frac{\partial C}{\partial y_i} = 4 \sum_j (p_{ij} - q_{ij})(y_i - y_j)(1 + \|y_i - y_j\|^2)^{-1}$$


## algoritmo t-sne

**Algoritmo 1: Versión Simple de t-Distributed Stochastic Neighbor Embedding.**

**Datos:** conjunto de datos $X = \{x_1, x_2,..., x_n\}$, parámetros de la función de costo: perplejidad $Perp$, parámetros de optimización: número de iteraciones $T$, tasa de aprendizaje $\eta$, momento $\alpha(t)$.

**Resultado:** representación de baja dimensión $Y^{(T)} = \{y_1, y_2,..., y_n\}$.

**Inicio:**

1.  calcular afinidades por pares $p_{j|i}$ con perplejidad $Perp$ (usando Ecuación 1)

2.  establecer $p_{ij} = \frac{p_{j|i} + p_{i|j}}{2n}$

3.  muestrear solución inicial $Y^{(0)} = \{y_1, y_2,..., y_n\}$ desde $\mathcal{N}(0,10^{-4}I)$

4.  para $t=1$ hasta $T$ hacer

    a.  calcular afinidades de baja dimensión $q_{ij}$ (usando Ecuación 4)
    b.  calcular gradiente $\frac{\partial C}{\partial Y}$ (usando Ecuación 5)
    c.  establecer $Y^{(t)} = Y^{(t-1)} + \eta\frac{\partial C}{\partial Y} + \alpha(t)(Y^{(t-1)} - Y^{(t-2)})$

5.  Fin

# ejemplo t-sne

```{python}
#| label: tsne-example
#| output: asis
# 1. Generar datos sintéticos
np.random.seed(42)
n_points = 100

cluster1 = np.random.normal(loc=[0, 0], scale=0.5, size=(n_points, 2))
cluster2 = np.random.normal(loc=[3, 3], scale=0.7, size=(n_points, 2))
cluster3 = np.random.normal(loc=[-3, 3], scale=0.6, size=(n_points, 2))

X = np.vstack([cluster1, cluster2, cluster3])
labels = np.array([0]*n_points + [1]*n_points + [2]*n_points)

# 2. Visualizar datos originales
plt.figure(figsize=(6, 6))
for i, (color, name) in enumerate(zip(DIGIT_COLORS[:3], ['Cluster 1', 'Cluster 2', 'Cluster 3'])):
    mask = labels == i
    plt.scatter(X[mask, 0], X[mask, 1], c=color, label=name, alpha=0.7)
plt.title('Datos Originales en R²', fontsize=14, fontweight='bold', pad=20)
plt.xlabel('X', fontsize=12)
plt.ylabel('Y', fontsize=12)
plt.legend()
plt.grid(True, alpha=0.3)
plt.tight_layout()
plt.show()

```

## ejemplo t-sne
```{python}
#| label: tsne-custom-functions
#| output: false
# Funciones auxiliares para t-SNE
def compute_p_ij(X, perplexity=30):
    """Calcular probabilidades en alta dimensión P_j|i"""
    n = X.shape[0]
    distances = np.sum((X[:, np.newaxis] - X[np.newaxis, :])**2, axis=2)
    p_ij = np.zeros((n, n))
    
    for i in range(n):
        beta = 1.0
        betamin = -np.inf
        betamax = np.inf
        
        for iteration in range(50):
            p_i = np.exp(-distances[i] * beta)
            p_i[i] = 0
            
            sum_pi = np.sum(p_i)
            if sum_pi == 0:
                p_i = np.ones(n) / n
                sum_pi = 1.0
            
            p_i = p_i / sum_pi
            
            entropy = -np.sum(p_i[p_i > 1e-10] * np.log2(p_i[p_i > 1e-10]))
            current_perplexity = 2**entropy
            
            perplexity_diff = current_perplexity - perplexity
            if abs(perplexity_diff) < 1e-5:
                break
                
            if perplexity_diff > 0:
                betamin = beta
                beta = (beta + betamax) / 2 if betamax != np.inf else beta * 2
            else:
                betamax = beta
                beta = (beta + betamin) / 2 if betamin != -np.inf else beta / 2
        
        p_ij[i] = p_i
    
    p_ij = (p_ij + p_ij.T) / (2 * n)
    p_ij = np.maximum(p_ij, 1e-12)
    
    return p_ij

def compute_q_ij(Y):
    """Calcular probabilidades en baja dimensión usando distribución t-Student"""
    n = Y.shape[0]
    distances_sq = np.sum((Y[:, np.newaxis] - Y[np.newaxis, :])**2, axis=2)
    
    q_ij = (1 + distances_sq)**(-1)
    np.fill_diagonal(q_ij, 0)
    
    sum_q = np.sum(q_ij)
    if sum_q > 0:
        q_ij = q_ij / sum_q
    else:
        q_ij = np.ones_like(q_ij) / (n * (n-1))
        np.fill_diagonal(q_ij, 0)
    
    return q_ij

def compute_gradient(p_ij, q_ij, Y):
    """Calcular gradiente de la función de costo KL"""
    n, dim = Y.shape
    gradient = np.zeros_like(Y)
    
    Y_diff = Y[:, np.newaxis, :] - Y[np.newaxis, :, :]
    distances_sq = np.sum(Y_diff**2, axis=2)
    inv_distances = (1 + distances_sq)**(-1)
    pq_diff = p_ij - q_ij
    
    for d in range(dim):
        gradient[:, d] = 4 * np.sum(
            pq_diff[:, :, np.newaxis] * Y_diff[:, :, d:d+1] * inv_distances[:, :, np.newaxis],
            axis=1
        ).flatten()
    
    return gradient

def tsne_custom(X, n_components=1, perplexity=30, iterations_to_save=[1, 10, 25, 50, 100, 250, 500, 1000], 
                learning_rate=200, early_exaggeration=12, momentum=0.8):
    """Implementación personalizada de t-SNE que guarda estados intermedios"""
    n_samples = X.shape[0]
    max_iter = max(iterations_to_save)
    
    Y = np.random.normal(0, 1e-4, (n_samples, n_components))
    Y_prev = Y.copy()
    
    p_ij = compute_p_ij(X, perplexity)
    p_ij_exag = p_ij * early_exaggeration
    
    results = {}
    
    for i in range(max_iter):
        current_p = p_ij_exag if i < 250 else p_ij
        q_ij = compute_q_ij(Y)
        gradient = compute_gradient(current_p, q_ij, Y)
        
        Y_new = Y - learning_rate * gradient + momentum * (Y - Y_prev)
        Y_prev = Y.copy()
        Y = Y_new
        
        current_iter = i + 1
        if current_iter in iterations_to_save:
            kl_div = np.sum(p_ij * np.log((p_ij + 1e-12) / (q_ij + 1e-12)))
            results[current_iter] = {
                'embedding': Y.copy().flatten(),
                'kl_divergence': kl_div
            }
    
    return results


```


```{python}
iterations_to_show = [1, 10, 25, 50, 100, 250, 500, 1000]
tsne_results = tsne_custom(X, n_components=1, iterations_to_save=iterations_to_show,
                          learning_rate=200, perplexity=15)
fig, axes = plt.subplots(2, 2, figsize=(15, 12))
axes = axes.flatten()

for idx, n_iter in enumerate(iterations_to_show[:4]):
    ax = axes[idx]
    Y_1d = tsne_results[n_iter]['embedding']
    kl_div = tsne_results[n_iter]['kl_divergence']
    
    for i, (color, name) in enumerate(zip(DIGIT_COLORS, ['Cluster 1', 'Cluster 2', 'Cluster 3'])):
        mask = labels == i
        ax.scatter(Y_1d[mask], np.zeros(np.sum(mask)), c=color, label=name if idx == 0 else "", 
                  alpha=0.4, s=25)
    
    for i in range(3):
        cluster_center = np.mean(Y_1d[labels == i])
        ax.axvline(cluster_center, color=DIGIT_COLORS[i], alpha=0.6, linestyle='--', linewidth=2)
        ax.text(cluster_center, 0.05, f'C{i+1}', ha='center', va='bottom', 
                color=DIGIT_COLORS[i], fontweight='bold', fontsize=10)
    
    ax.set_title(f'Iteración {n_iter}\nKL div: {kl_div:.3f}', fontsize=12, fontweight='bold')
    ax.set_xlabel('Coordenada t-SNE (R¹)', fontsize=10)
    ax.set_ylabel('')
    ax.set_yticks([])
    ax.set_ylim(-0.1, 0.1)
    ax.grid(True, alpha=0.3, axis='x')
    
    if idx == 0:
        ax.legend(bbox_to_anchor=(1.05, 1), loc='upper left')

plt.suptitle('Evolución de t-SNE Personalizado (Iteraciones 1-50)', fontsize=14, fontweight='bold', y=1.02)
plt.tight_layout()
plt.show()               
```

## ejemplo t-sne
```{python}
#| label: tsne-visualization-2
#| output: asis
# Visualizar últimas 4 iteraciones
fig, axes = plt.subplots(2, 2, figsize=(15, 12))
axes = axes.flatten()

for idx, n_iter in enumerate(iterations_to_show[4:]):
    ax = axes[idx]
    Y_1d = tsne_results[n_iter]['embedding']
    kl_div = tsne_results[n_iter]['kl_divergence']
    
    for i, (color, name) in enumerate(zip(DIGIT_COLORS, ['Cluster 1', 'Cluster 2', 'Cluster 3'])):
        mask = labels == i
        ax.scatter(Y_1d[mask], np.zeros(np.sum(mask)), c=color, label=name if idx == 0 else "", 
                  alpha=0.4, s=25)
    
    for i in range(3):
        cluster_center = np.mean(Y_1d[labels == i])
        ax.axvline(cluster_center, color=DIGIT_COLORS[i], alpha=0.6, linestyle='--', linewidth=2)
        ax.text(cluster_center, 0.05, f'C{i+1}', ha='center', va='bottom', 
                color=DIGIT_COLORS[i], fontweight='bold', fontsize=10)
    
    ax.set_title(f'Iteración {n_iter}\nKL div: {kl_div:.3f}', fontsize=12, fontweight='bold')
    ax.set_xlabel('Coordenada t-SNE (R¹)', fontsize=10)
    ax.set_ylabel('')
    ax.set_yticks([])
    ax.set_ylim(-0.1, 0.1)
    ax.grid(True, alpha=0.3, axis='x')
    
    if idx == 0:
        ax.legend(bbox_to_anchor=(1.05, 1), loc='upper left')

plt.suptitle('Evolución de t-SNE Personalizado (Iteraciones 100-1000)', fontsize=14, fontweight='bold', y=1.02)
plt.tight_layout()
plt.show()
```



# UMAP

UMAP (Uniform Manifold Approximation and Projection) trasciende la mera visualización de datos. Es un algoritmo robusto diseñado para **preservar la estructura topológica intrínseca** de datos de alta dimensión al proyectarlos a un espacio de menor dimensionalidad (e.g., 2D o 3D).

La **estructura topológica** se refiere a las propiedades de conectividad de los datos, como la formación de componentes conectados, ciclos o agujeros. UMAP aborda esta tarea construyendo una representación topológica en el espacio de alta dimensión y optimizando una proyección en baja dimensión para que esta representación sea lo más fiel posible.

---

## Conceptos Clave: Los Símplices

Para comprender el fundamento de UMAP, es esencial introducir el concepto de **símplex**, que son los bloques de construcción elementales en la topología algebraica:

* Un **0-símplex** es un vértice (un punto).
* Un **1-símplex** es una arista (una línea que conecta dos puntos).
* Un **2-símplex** es un triángulo (que conecta tres puntos).
* Un **3-símplex** es un tetraedro (que conecta cuatro puntos).

Esta jerarquía se extiende a dimensiones superiores. Un **complejo simplicial** es una colección de estos símplices que se interconectan de manera coherente, aproximando la forma topológica subyacente de los datos.

## **Tipos de Símplices**
![](./img/img1.webp){fig-align="center"}

---

## De los Datos Discretos a una Cubierta Adaptativa {.smaller}

Consideremos un conjunto de datos en un espacio de alta dimensión. La hipótesis fundamental de UMAP es que estos datos son un muestreo de una variedad (manifold) subyacente de menor dimensionalidad. El primer paso computacional es construir una "cubierta" de esta variedad mediante conjuntos abiertos, que pueden ser visualizados como "bolas" alrededor de cada punto.

Observemos un conjunto de datos bidimensional de ejemplo, una "onda sinusoidal ruidosa":

![](./img/img2.webp){fig-align="center"}

A diferencia de métodos que emplean un radio fijo para estos conjuntos abiertos, UMAP **adapta el tamaño de cada conjunto abierto** basándose en la densidad local de los datos.

## Cubierta con Radios Fijos
![](./img/img3.webp){fig-align="center"}
<figcaption>
Esta figura (<code>img3.webp</code>) ilustra una cubierta generada con radios fijos. Se observa que esta aproximación es ineficaz para capturar la estructura local, ya que no se ajusta a las variaciones de densidad, resultando en coberturas inadecuadas en regiones dispersas y solapamientos excesivos en regiones densas.
</figcaption>

Una cubierta con un radio globalmente fijo puede llevar a una representación imprecisa de la topología local de la variedad.

---

## Uniformidad Local

Para garantizar que todas las vecindades locales muestren una **uniformidad de muestreo aparente**, UMAP implementa una re-escalación adaptativa de la métrica en cada vecindad. Esto se conceptualiza como un ajuste del "volumen" efectivo de la vecindad de $k$ puntos alrededor de cada dato.

Este ajuste métrico se logra calculando un valor $\sigma_i$ (sigma local) para cada punto $X_i$. Este $\sigma_i$ actúa como un factor de escala. Se determina iterativamente para satisfacer la siguiente condición:

$$\sum_{j=1}^{k} \exp\left(-\frac{d(X_i, X_j) - \rho_i}{\sigma_i}\right) = \log_2(k)$$

## Uniformidad Local{.smaller}
$$\sum_{j=1}^{k} \exp\left(-\frac{d(X_i, X_j) - \rho_i}{\sigma_i}\right) = \log_2(k)$$

Desglosemos los términos:

* $X_i$: El **punto de datos focal** bajo consideración.
* $X_j$: Los **$k$ vecinos más cercanos** a $X_i$.
* $d(X_i, X_j)$: La **distancia euclidiana** (o la métrica seleccionada) entre $X_i$ y cada vecino $X_j$ en el espacio de alta dimensión.
* $\rho_i$ (rho): La **distancia al primer vecino no cero** de $X_i$, es decir, la distancia más pequeña a cualquier otro punto.
* $\sigma_i$ (sigma): El **parámetro de escala local** calculado para cada $X_i$. Un $\sigma_i$ pequeño indica una compresión efectiva de las distancias (región densa), mientras que un $\sigma_i$ grande indica un estiramiento (región dispersa).
* $\log_2(k)$: Un **valor objetivo constante**. Asegura que, en promedio, la suma de las probabilidades de conexión de los $k$ vecinos de $X_i$ sea $\log_2(k)$, normalizando la "conectividad efectiva" de cada punto.

**Impacto:** Este procedimiento resulta en una **distribución uniforme local simulada**, garantizando que las vecindades de los puntos se comporten de manera consistente, independientemente de la densidad original de los datos.

## Cubierta Adaptada por Densidad 
![](./img/img6.webp){fig-align="center"}
<figcaption>
Esta figura muestra la onda sinusoidal con los conjuntos abiertos ajustados por la densidad. Los radios son menores en regiones densas y mayores en regiones dispersas, logrando una "cobertura uniforme" de la variedad en términos de conectividad local.
</figcaption>

---

## La Métrica Adaptada: Visualizando la Uniformización Local

El ajuste dinámico de $\sigma_i$ y $\rho_i$ produce una métrica localmente adaptada que hace que los puntos se "sientan" uniformemente distribuidos. Conceptualmente, esto puede visualizarse como una deformación del espacio alrededor de cada punto, donde las bolas de radios fijos se "estiran" o "comprimen" para lograr una densidad efectiva constante.

## Espacio Métrico Adaptado{.smaller}
![](./img/img8.webp){fig-align="center"}
<figcaption>
Esta figura representa cómo UMAP ajusta dinámicamente las distancias entre los puntos (visualizado como el "estiramiento" o "compresión" de las regiones espaciales) para lograr una densidad efectiva uniforme en todo el espacio de datos. Los puntos en regiones más densas se "separan" efectivamente de sus vecinos, mientras que los puntos en regiones más dispersas se "acercan" artificialmente, nivelando la percepción de la densidad local.
</figcaption>
Este ajuste es fundamental para que el "Fuzzy Simplicial Set" final refleje con precisión la topología subyacente, independientemente de las variaciones de densidad intrínsecas del dataset original.

---

## Construcción del Grafo Ponderado

Una vez que los parámetros de escala local ($\sigma_i$ y $\rho_i$) han sido determinados para cada punto, UMAP calcula los "scores de similaridad" entre cada punto $X_i$ y sus vecinos $X_j$. Estos scores representan la **fuerza de conexión** probabilística entre los puntos:

$$s_{ij} = \exp\left(-\frac{d(X_i, X_j) - \rho_i}{\sigma_i}\right)$$

Aquí, $s_{ij}$ es el **score de similaridad** (o "probabilidad de conexión") de $X_i$ a $X_j$. Un valor cercano a 1 indica una conexión fuerte.

Estos scores se utilizan para construir un **grafo de conectividad ponderado global**. Cada punto de datos se convierte en un nodo del grafo, y las conexiones entre $X_i$ y $X_j$ son aristas con un peso $W_{ij}$. Este grafo se conceptualiza como un **Conjunto Simplicial Difuso (Fuzzy Simplicial Set)**.

## Grafo de Conectividad 
![](./img/img4.webp){fig-align="center"}
<figcaption>
Esta figura muestra el complejo simplicial construido a partir de las conexiones (aristas) y posibles símplices de orden superior (regiones sombreadas) sobre la onda sinusoidal, representando la estructura topológica inferida.
</figcaption>

---

## Fuzzy Simplicial Set

La característica "difusa" (fuzzy) de los Conjuntos Simpliciales radica en que las conexiones no son binarias (sí/no), sino que poseen un **grado de pertenencia** (un valor continuo entre 0 y 1). Para consolidar los scores $s_{ij}$ y $s_{ji}$ (ya que la similaridad de $X_i$ a $X_j$ puede diferir de $X_j$ a $X_i$ debido a sus $\sigma$ y $\rho$ individuales), UMAP emplea la siguiente fórmula:

$$W_{ij} = s_{ij} + s_{ji} - (s_{ij} \cdot s_{ji})$$

Análisis de la fórmula:

* $W_{ij}$: El **peso final de la arista** entre $X_i$ y $X_j$ en el grafo de conectividad global.
* $s_{ij}$: El score de similaridad calculado de $X_i$ a $X_j$.
* $s_{ji}$: El score de similaridad calculado de $X_j$ a $X_i$.


## Matriz de Similaridad

| Punto | $P_1$ | $P_2$ | $P_3$ | $P_4$ | $P_5$ |
| :---- | :---- | :---- | :---- | :---- | :---- |
| $P_1$ | 1.00  | 0.95  | 0.00  | 0.00  | 0.00  |
| $P_2$ | 0.95  | 1.00  | 0.82  | 0.00  | 0.00  |
| $P_3$ | 0.00  | 0.82  | 1.00  | 0.71  | 0.00  |
| $P_4$ | 0.00  | 0.00  | 0.71  | 1.00  | 0.90  |
| $P_5$ | 0.00  | 0.00  | 0.00  | 0.90  | 1.00  |

<figcaption>
Esta tabla ilustra una porción conceptual de la matriz de similaridad dispersa. Los valores entre 0 y 1 (donde 1.00 representa conectividad total y 0.00 ausencia de conectividad) indican la fuerza de la relación entre los puntos. Las entradas de 0.00 (o muy cercanas a cero) muestran la naturaleza dispersa de la matriz, donde solo los vecinos cercanos tienen un peso significativo.
</figcaption>


---

## Representación en Baja Dimensión

Una vez que UMAP ha construido el **Conjunto Simplicial Difuso (Fuzzy Simplicial Set)** en el espacio de alta dimensión, que encapsula la estructura topológica y las similaridades probabilísticas ($W_{ij}$), el siguiente paso es encontrar una representación de estos datos en un espacio de menor dimensionalidad (e.g., $\mathbb{R}^2$ o $\mathbb{R}^3$).

El objetivo es generar una incrustación $Y = \{y_1, \dots, y_N\} \subset \mathbb{R}^d$ (donde $d$ es la dimensión objetivo, típicamente 2 o 3) que sea topológicamente similar al grafo de alta dimensión. Esto se logra formulando un problema de optimización para preservar las **conectividades probabilísticas** establecidas en el espacio original. La meta es que si dos puntos $X_i$ y $X_j$ tienen una alta probabilidad de conexión $W_{ij}$ en alta dimensión, sus correspondientes incrustaciones $y_i$ y $y_j$ deben estar cerca en baja dimensión, y viceversa.

---

## Modelado de la Similitud en Baja Dimensión 

Para cuantificar la "similaridad" o "conectividad" entre los puntos $y_i$ y $y_j$ en el espacio de baja dimensión, UMAP emplea una función de kernel específica, elegida por sus propiedades deseables para la optimización y la visualización:

$$w_{ij} = \frac{1}{1 + a(d(y_i, y_j))^ {2b}}$$

Analicemos los componentes de esta fórmula y su propósito:

* $w_{ij}$: Representa la **similaridad o probabilidad de conexión** entre las proyecciones $y_i$ y $y_j$ en el espacio de baja dimensión. Este es el homólogo de $W_{ij}$ pero definido en el espacio de salida.
* $d(y_i, y_j)$: Es la **distancia euclidiana** entre $y_i$ y $y_j$ en el espacio de baja dimensión.
* $a$ y $b$: Son **parámetros de UMAP** derivados de los hiperparámetros `min_dist` y `spread`.
---

## Función de costos

La función objetivo que UMAP busca minimizar es:
$$L(Y) = \sum_{i \neq j} \left[ W_{ij} \log\left(\frac{W_{ij}}{w_{ij}}\right) + (1 - W_{ij}) \log\left(\frac{1 - W_{ij}}{1 - w_{ij}}\right) \right]$$

Donde:

* $W_{ij}$: La **probabilidad de conexión** entre $X_i$ y $X_j$ en el espacio de **alta dimensión** (un valor entre 0 y 1).

* $w_{ij}$: La **probabilidad de conexión** entre $y_i$ y $y_j$ en el espacio de **baja dimensión** (calculada con el kernel de Cauchy).

1.  **Término de Atracción (primer término):** $W_{ij} \log\left(\frac{W_{ij}}{w_{ij}}\right)$

2.  **Término de Repulsión (segundo término):** $(1 - W_{ij}) \log\left(\frac{1 - W_{ij}}{1 - w_{ij}}\right)$


---

## Descenso de Gradiente Estocástico 

1.  **Inicialización del Embedding:**
    * Para acelerar la convergencia y proporcionar un punto de partida razonable, los puntos $y_i$ en el espacio de baja dimensión se inicializan comúnmente utilizando una **incrustación espectral** (e.g., basada en los vectores propios del Laplaciano normalizado del grafo de conectividad de alta dimensión). Alternativamente, se puede usar una inicialización aleatoria.

2.  **Muestreo de Aristas y No-Aristas:**
    * En cada iteración del SGD, se selecciona aleatoriamente una arista (un par $(i, j)$ con $W_{ij} > 0$) de la matriz de similaridad en alta dimensión. Para este par, se aplica una **fuerza de atracción** a $y_i$ y $y_j$.
    * Para la repulsión, UMAP emplea **muestreo negativo**. En lugar de considerar todos los pares no conectados (lo que sería computacionalmente inviable para grandes datasets, $O(N^2)$), se muestrean aleatoriamente varios puntos $y_k$ que no están conectados a $y_i$. Para estos pares $(i, k)$, se aplica una **fuerza de repulsión**. La proporción de muestras negativas por muestra positiva es un parámetro configurable.

## Descenso de Gradiente Estocástico 

3.  **Cálculo de Gradientes y Actualización de Posiciones:**
    * Se calculan las derivadas parciales de la función de pérdida $L$ con respecto a las coordenadas de los puntos $y_i$ y $y_j$.
    * Estas gradientes determinan la dirección y magnitud del ajuste de las posiciones de los puntos.
    * Las posiciones de los puntos se actualizan iterativamente: $y_k \leftarrow y_k - \eta \nabla_{y_k} L$, donde $\eta$ es la tasa de aprendizaje.

4.  **Annealing y Convergencia:**
    * El proceso de optimización se ejecuta durante un número predefinido de épocas (`n_epochs`).
    * A menudo, las fuerzas repulsivas se ponderan más fuertemente al inicio de la optimización para asegurar que los puntos se dispersen adecuadamente y no queden atrapados en mínimos locales apretados. A medida que avanza la optimización, el equilibrio entre atracción y repulsión se ajusta para permitir que las agrupaciones se refinen.


## Hiperparámetros

* **`n_neighbors` (Número de vecinos):** Este parámetro (usado en la fase de alta dimensión para construir el k-NN graph) tiene un impacto crucial en la balance entre la preservación de la estructura local y global en la incrustación final.
    * **Valores pequeños:** Enfatizan la estructura local, lo que puede llevar a una fragmentación de la incrustación y a la aparición de muchos clústeres pequeños y separados.
    * **Valores grandes:** Permiten que UMAP considere más la estructura global de los datos, lo que puede resultar en una vista más cohesiva, pero quizás menos detallada de las relaciones locales.

## Hiperparámetros

* **`min_dist` (Distancia Mínima):** Directamente relacionado con el parámetro `a` del kernel de Cauchy.
    * Controla cuán cerca se pueden agrupar los puntos en la incrustación de baja dimensión.
    * **`min_dist` pequeño (cercano a 0):** Los puntos pueden agruparse de forma muy densa, lo que es útil para visualizar clústeres bien separados y compactos.
    * **`min_dist` grande:** Los puntos son forzados a estar más dispersos, lo que puede revelar una estructura más continua y fluida, o separar clústeres que de otra manera se solaparían visualmente.

## Hiperparámetros

* **`spread` (Dispersión):** Directamente relacionado con el parámetro `b` del kernel de Cauchy.
    * Controla la dispersión general del embedding.
    * **`spread` pequeño:** Resulta en una incrustación más compacta.
    * **`spread` grande:** Permite que la incrustación se extienda más, útil para visualizar la jerarquía global.

## Hiperparámetros

* **`n_epochs` (Número de Épocas):**
    * Define el número de iteraciones del algoritmo de optimización SGD.
    * Más épocas (`n_epochs` más alto) generalmente conducen a una incrustación más optimizada y estable, pero a costa de un mayor tiempo de cómputo.
    * Para datasets grandes o embeddings de alta calidad, un `n_epochs` elevado es recomendable.

Estos parámetros permiten al usuario ajustar el equilibrio entre la preservación de la estructura local y global, y la densidad visual de la incrustación resultante, adaptando UMAP a diversas necesidades de análisis y visualización.

## Proyección de Datos No Vistos en UMAP

1.  **Cálculo de Distancias a los Vecinos más Cercanos Aprendidos:** Para cada nuevo punto $x_{\text{new},i} \in X_{\text{new}}$, UMAP identifica sus $k$ vecinos más cercanos dentro del conjunto de datos de entrenamiento original $X_{\text{train}}$. Se calculan las distancias $d(x_{\text{new},i}, x_j)$ a estos vecinos.

## Proyección de Datos No Vistos en UMAP

2.  **Inferencia de Probabilidades de Conexión:** Utilizando los parámetros $\sigma_j$ y $\rho_j$ aprendidos de los vecinos del conjunto de entrenamiento, se infieren las probabilidades de conexión para el nuevo punto con sus vecinos más cercanos. Esto se asemeja al cálculo de $s_{ij}$ en la fase de construcción del grafo:
    $$s_{\text{new},ij} = \exp\left(-\frac{d(x_{\text{new},i}, x_j) - \rho_j}{\sigma_j}\right)$$
    donde $x_j$ son los vecinos de $x_{\text{new},i}$ en $X_{\text{train}}$.

## Proyección de Datos No Vistos en UMAP

3.  **Localización en el Espacio de Baja Dimensión:** El nuevo punto $y_{\text{new},i}$ se inserta en el espacio de baja dimensión de tal manera que sus probabilidades de conexión con los puntos de entrenamiento cercanos $y_j$ (ya incrustados) sean lo más similares posible a las probabilidades inferidas en alta dimensión. Esto se logra minimizando una función de costo similar a la utilizada en la fase de entrenamiento, pero manteniendo fijos los puntos del conjunto de entrenamiento $y_j$.
    $$L(y_{\text{new},i}) = \sum_{j \in \text{vecinos}(x_{\text{new},i})} \left[ s_{\text{new},ij} \log\left(\frac{s_{\text{new},ij}}{w_{\text{new},ij}}\right) + (1 - s_{\text{new},ij}) \log\left(\frac{1 - s_{\text{new},ij}}{1 - w_{\text{new},ij}}\right) \right]$$
    donde $w_{\text{new},ij}$ es la probabilidad de conexión en baja dimensión entre $y_{\text{new},i}$ y $y_j$, calculada con el kernel de Cauchy:
    $$w_{\text{new},ij} = \frac{1}{1 + a(d(y_{\text{new},i}, y_j))^ {2b}}$$

# Aplicaciones

## t-SNE

### Proceso de Optimización de t-SNE

```{python}
#| label: load-data
#| output: asis

# Cargar el dataset de dígitos
digits = datasets.load_digits(n_class=10)
X = digits.data  
y = digits.target 

```
```{python}
#| label: optimization-visualization
#| output: asis

def plot_optimization_process(X, y, n_steps=6):
    """Visualiza el proceso de optimización de t-SNE en diferentes iteraciones."""
    n_iter = 1000
    fig, axes = plt.subplots(2, 3, figsize=(15, 10))
    axes = axes.flatten()

    iterations_to_plot = np.linspace(250, n_iter, n_steps).astype(int)
    if iterations_to_plot[-1] < 250:
        iterations_to_plot[-1] = 250
    
    for i, iter in enumerate(iterations_to_plot):
        tsne = TSNE(n_components=2, 
                   init='random',
                   random_state=42,
                   perplexity=30,
                   n_iter=iter)
        
        Y = tsne.fit_transform(X)
        
        ax = axes[i]
        for digit in range(10):
            mask = y == digit
            ax.scatter(Y[mask, 0], Y[mask, 1], 
                      c=[DIGIT_COLORS[digit]], 
                      label=f'{digit}' if i == 0 else "")
        
        ax.set_title(f'Iteración {iter}')
        ax.set_xlabel('t-SNE 1')
        ax.set_ylabel('t-SNE 2')
        ax.set_aspect('equal')
        ax.grid(True, alpha=0.3)
        
        if i == 0:
            ax.legend(bbox_to_anchor=(1.05, 1), loc='upper left')
    
    plt.suptitle('Evolución de la Proyección t-SNE', fontsize=14, fontweight='bold', y=1.02)
    plt.tight_layout()
    plt.show()
plot_optimization_process(X, y)
```

## Hiperparámetros t-SNE

```{python}
#| label: hyperparameter-effects
#| output: asis

def plot_perplexity_effect(X, y):
    """Visualiza el efecto de diferentes perplejidades en t-SNE."""
    perplexities = [5, 30, 50]
    fig, axes = plt.subplots(1, 3, figsize=(15, 5))
    
    for i, perp in enumerate(perplexities):
        tsne = TSNE(n_components=2, 
                   init='random',
                   random_state=42,
                   perplexity=perp)
        Y = tsne.fit_transform(X)
        
        ax = axes[i]
        for digit in range(10):
            mask = y == digit
            ax.scatter(Y[mask, 0], Y[mask, 1], 
                      c=[DIGIT_COLORS[digit]], 
                      label=f'{digit}' if i == 0 else "")
        ax.set_title(f'Perplexity = {perp}')
        ax.set_xlabel('t-SNE 1')
        ax.set_ylabel('t-SNE 2')
        ax.set_aspect('equal')
        ax.grid(True, alpha=0.3)
        
        if i == 0:
            ax.legend(bbox_to_anchor=(1.05, 1), loc='upper left')
    
    plt.suptitle('Efecto de la Perplejidad en t-SNE', fontsize=14, fontweight='bold', y=1.02)
    plt.tight_layout()
    plt.show()

def plot_learning_rate_effect(X, y):
    """Visualiza el efecto de diferentes tasas de aprendizaje en t-SNE."""
    learning_rates = [50, 200, 1000]
    fig, axes = plt.subplots(1, 3, figsize=(15, 5)) # Cambiado a figsize=(15, 5) para que los gráficos de learning rate también tengan 3 subplots en una fila y sean más grandes
    
    for i, lr in enumerate(learning_rates):
        tsne = TSNE(n_components=2, 
                   init='random',
                   random_state=42,
                   learning_rate=lr)
        Y = tsne.fit_transform(X)
        
        ax = axes[i]
        for digit in range(10):
            mask = y == digit
            ax.scatter(Y[mask, 0], Y[mask, 1], 
                      c=[DIGIT_COLORS[digit]], 
                      label=f'{digit}' if i == 0 else "")
        ax.set_title(f'Learning Rate = {lr}')
        ax.set_xlabel('t-SNE 1')
        ax.set_ylabel('t-SNE 2')
        ax.set_aspect('equal')
        ax.grid(True, alpha=0.3)
        
        if i == 0:
            ax.legend(bbox_to_anchor=(1.05, 1), loc='upper left')
    
    plt.suptitle('Efecto del Learning Rate en t-SNE', fontsize=14, fontweight='bold', y=1.02)
    plt.tight_layout()
    plt.show()


plot_perplexity_effect(X, y)
```

## Hiperparámetros t-SNE

```{python}
#| label: hyperparameter-effectsa
#| output: asis
plot_learning_rate_effect(X, y)
```

## UMAP
```{python}
#| label: umap-optimization-process
#| output: asis

def plot_umap_optimization_process(X, y, n_steps=6):
    """Visualiza el proceso de optimización de UMAP en diferentes iteraciones."""
    
    fig, axes = plt.subplots(2, 3, figsize=(15, 10))
    axes = axes.flatten()
    
    # Iteraciones a visualizar
    epochs_to_plot = [50, 100, 200, 300, 500, 1000]
    
    with warnings.catch_warnings():
        warnings.simplefilter("ignore")
        
        for i, n_epochs in enumerate(epochs_to_plot):
            # Crear instancia de UMAP con parámetros consistentes
            umap_model = UMAP(
                n_components=2,
                n_neighbors=15,
                min_dist=0.1,
                n_epochs=n_epochs,
                random_state=42,
                verbose=False
            )
            
            Y = umap_model.fit_transform(X)
            
            # Crear scatter plot usando DIGIT_COLORS
            ax = axes[i]
            for digit in range(10):
                mask = y == digit
                ax.scatter(Y[mask, 0], Y[mask, 1], 
                          c=[DIGIT_COLORS[digit]], 
                          label=f'{digit}' if i == 0 else "")
            
            ax.set_title(f'Interación: {n_epochs}', fontsize=12, fontweight='bold')
            ax.set_xlabel('UMAP 1', fontsize=10)
            ax.set_ylabel('UMAP 2', fontsize=10)
            ax.set_aspect('equal')
            ax.grid(True, alpha=0.3)
            
            if i == 0:
                ax.legend(bbox_to_anchor=(1.05, 1), loc='upper left')
    
    plt.suptitle('Evolución de la Proyección UMAP por Épocas', fontsize=14, fontweight='bold', y=1.02)
    plt.tight_layout()
    plt.show()
```

```{python}
#| label: umap-implementation
#| output: false

import numpy as np
from sklearn.decomposition import PCA

def compute_high_dim_probabilities(X, n_neighbors=15, min_dist=0.1):
    """Calcular probabilidades en alta dimensión usando UMAP"""
    n_samples = X.shape[0]
    distances = np.sum((X[:, np.newaxis] - X[np.newaxis, :])**2, axis=2)
    knn_indices = np.argsort(distances, axis=1)[:, 1:n_neighbors+1]
    knn_distances = np.array([distances[i, knn_indices[i]] for i in range(n_samples)])
    sigmas = np.zeros(n_samples)
    for i in range(n_samples):
        sigma_min = 0
        sigma_max = 1000
        target = np.log2(n_neighbors)
        for _ in range(50):
            sigma = (sigma_min + sigma_max) / 2
            n_neighbors_est = np.sum(np.exp(-knn_distances[i] / sigma))
            if n_neighbors_est > target:
                sigma_max = sigma
            else:
                sigma_min = sigma
        sigmas[i] = sigma
    p_ij = np.zeros((n_samples, n_samples))
    for i in range(n_samples):
        p_ij[i, knn_indices[i]] = np.exp(-knn_distances[i] / sigmas[i])
    p_ij = p_ij + p_ij.T - p_ij * p_ij.T
    p_ij = p_ij / np.sum(p_ij)
    return p_ij

def compute_low_dim_probabilities(Y, min_dist=0.1):
    """Calcular probabilidades en baja dimensión usando UMAP"""
    n_samples = Y.shape[0]
    distances = np.sum((Y[:, np.newaxis] - Y[np.newaxis, :])**2, axis=2)
    a = 1.0
    b = 1.0
    q_ij = 1 / (1 + a * distances**b)
    np.fill_diagonal(q_ij, 0)
    q_ij = q_ij / np.sum(q_ij)
    return q_ij

def compute_umap_gradient(p_ij, q_ij, Y):
    n_samples, n_components = Y.shape
    gradient = np.zeros_like(Y)
    Y_diff = Y[:, np.newaxis, :] - Y[np.newaxis, :, :]
    term1 = p_ij * (1 - q_ij)
    term2 = (1 - p_ij) * q_ij
    for d in range(n_components):
        gradient[:, d] = 2 * np.sum(
            (term1 - term2)[:, :, np.newaxis] * Y_diff[:, :, d:d+1],
            axis=1
        ).flatten()
    return gradient

def umap_custom(X, n_components=1, n_neighbors=15, min_dist=0.1, 
                learning_rate=1.0, n_epochs=1000, iterations_to_save=[1, 10, 25, 50, 100, 250, 500, 1000]):
    n_samples = X.shape[0]
    max_iter = max(iterations_to_save)
    pca = PCA(n_components=n_components)
    Y = pca.fit_transform(X) * 0.1
    p_ij = compute_high_dim_probabilities(X, n_neighbors, min_dist)
    momentum = 0.8
    Y_prev = Y.copy()
    results = {}
    for epoch in range(max_iter):
        current_lr = learning_rate * (1 - epoch/max_iter)
        q_ij = compute_low_dim_probabilities(Y, min_dist)
        gradient = compute_umap_gradient(p_ij, q_ij, Y)
        Y_new = Y - current_lr * gradient + momentum * (Y - Y_prev)
        Y_prev = Y.copy()
        Y = Y_new
        current_iter = epoch + 1
        if current_iter in iterations_to_save:
            cross_entropy = -np.sum(p_ij * np.log(q_ij + 1e-12) + 
                                  (1 - p_ij) * np.log(1 - q_ij + 1e-12))
            results[current_iter] = {
                'embedding': Y.copy().flatten(),
                'cross_entropy': cross_entropy
            }
    return results

def plot_umap_convergence_metrics(X, y):
    """Visualiza las métricas de convergencia de UMAP."""
    
    n_epochs = 1000
    step_size = 50
    epochs = range(50, n_epochs + 1, step_size)
    
    # Métricas a seguir
    distance_correlations = []
    silhouette_scores = []
    
    with warnings.catch_warnings():
        warnings.simplefilter("ignore")
        
        for epoch in epochs:
            umap_model = UMAP(
                n_components=2,
                n_neighbors=15,
                min_dist=0.1,
                n_epochs=epoch,
                random_state=42,
                verbose=False
            )
            
            Y = umap_model.fit_transform(X)
            
            # Correlación de distancias
            dist_X = euclidean_distances(X)
            dist_Y = euclidean_distances(Y)
            corr, _ = spearmanr(dist_X.flatten(), dist_Y.flatten())
            distance_correlations.append(corr)
            
            # Silhouette score
            sil_score = silhouette_score(Y, y)
            silhouette_scores.append(sil_score)
    
    # Visualizar métricas
    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))
    
    # Correlación de distancias
    ax1.plot(epochs, distance_correlations, 'b-', label='Correlación de Distancias')
    ax1.set_title('Evolución de la Correlación de Distancias')
    ax1.set_xlabel('Época')
    ax1.set_ylabel('Correlación de Spearman')
    ax1.grid(True, alpha=0.3)
    
    # Silhouette score
    ax2.plot(epochs, silhouette_scores, 'r-', label='Silhouette Score')
    ax2.set_title('Evolución del Silhouette Score')
    ax2.set_xlabel('Época')
    ax2.set_ylabel('Silhouette Score')
    ax2.grid(True, alpha=0.3)
    
    plt.suptitle('Métricas de Convergencia de UMAP')
    plt.tight_layout()
    plt.show()

def plot_umap_n_neighbors_effect(X, y):
    """Visualiza el efecto de diferentes valores de n_neighbors en UMAP."""
    n_neighbors_values = [5, 15, 50]
    fig, axes = plt.subplots(1, 3, figsize=(15, 5))
    
    with warnings.catch_warnings():
        warnings.simplefilter("ignore")
        
        for i, n_neighbors in enumerate(n_neighbors_values):
            umap_model = UMAP(
                n_components=2,
                n_neighbors=n_neighbors,
                min_dist=0.1,
                random_state=42,
                verbose=False
            )
            
            Y = umap_model.fit_transform(X)
            
            ax = axes[i]
            for digit in range(10):
                mask = y == digit
                ax.scatter(Y[mask, 0], Y[mask, 1], 
                           c=[DIGIT_COLORS[digit]], 
                           label=f'{digit}' if i == 0 else "")
            
            ax.set_title(f'n_neighbors = {n_neighbors}')
            ax.set_xlabel('UMAP 1')
            ax.set_ylabel('UMAP 2')
            ax.set_aspect('equal')  # Hacer el gráfico cuadrado
            
            if i == 0:
                ax.legend(bbox_to_anchor=(1.05, 1), loc='upper left')
    
    plt.suptitle('Efecto del Parámetro n_neighbors')
    plt.tight_layout()
    plt.show()

def plot_umap_min_dist_effect(X, y):
    """Visualiza el efecto de diferentes valores de min_dist en UMAP."""
    min_dist_values = [0.01, 0.1, 0.5]
    fig, axes = plt.subplots(1, 3, figsize=(15, 5))
    
    with warnings.catch_warnings():
        warnings.simplefilter("ignore")
        
        for i, min_dist in enumerate(min_dist_values):
            umap_model = UMAP(
                n_components=2,
                n_neighbors=15,
                min_dist=min_dist,
                random_state=42,
                verbose=False
            )
            
            Y = umap_model.fit_transform(X)
            
            ax = axes[i]
            for digit in range(10):
                mask = y == digit
                ax.scatter(Y[mask, 0], Y[mask, 1], 
                           c=[DIGIT_COLORS[digit]], 
                           label=f'{digit}' if i == 0 else "")
            
            ax.set_title(f'min_dist = {min_dist}')
            ax.set_xlabel('UMAP 1')
            ax.set_ylabel('UMAP 2')
            ax.set_aspect('equal')  # Hacer el gráfico cuadrado
            
            if i == 0:
                ax.legend(bbox_to_anchor=(1.05, 1), loc='upper left')
    
    plt.suptitle('Efecto del Parámetro min_dist')
    plt.tight_layout()
    plt.show()

iterations_to_show = [1, 10, 25, 50, 100, 250, 500, 1000]
umap_results = umap_custom(X, n_components=1, n_neighbors=15, min_dist=0.1,
                          learning_rate=1.0, n_epochs=1000, 
                          iterations_to_save=iterations_to_show)
```


```{python}
#| label: charts-umap
#| output: asis
    plot_umap_optimization_process(X, y)
```

## Hiperparámetros UMAP

```{python}
#| label: hyperparameter-effects-n-neighbors
#| output: asis
plot_umap_n_neighbors_effect(X, y)
```


## Hiperparámetros UMAP

```{python}
#| label: hyperparameter-effects-min-dist
#| output: asis
plot_umap_min_dist_effect(X, y)
```

# UMAP para datos no vistos

```{python}
#| label: umap-for-unseen-data
#| output: asis

from sklearn.model_selection import train_test_split

# Dividir el dataset en conjuntos de entrenamiento y prueba
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Entrenar UMAP en el conjunto de entrenamiento
umap_model = UMAP(n_components=2, n_neighbors=15, min_dist=0.1, n_epochs=1000, random_state=42)
Y_train_umap = umap_model.fit_transform(X_train)

# Transformar el conjunto de prueba usando el modelo entrenado
Y_test_umap = umap_model.transform(X_test)

# Visualizar los resultados de entrenamiento y prueba
fig, axes = plt.subplots(1, 2, figsize=(15, 6))

# Gráfico para el conjunto de entrenamiento
ax1 = axes[0]
for digit in range(10):
    mask = y_train == digit
    ax1.scatter(Y_train_umap[mask, 0], Y_train_umap[mask, 1],
                c=DIGIT_COLORS[digit],
                label=f'{digit}',
                alpha=0.7, s=25)
ax1.set_title('Proyección UMAP: Conjunto de Entrenamiento', fontsize=14, fontweight='bold', pad=15)
ax1.set_xlabel('UMAP 1', fontsize=12)
ax1.set_ylabel('UMAP 2', fontsize=12)
ax1.set_aspect('equal')
ax1.grid(True, alpha=0.3)
ax1.legend(title="Dígito", bbox_to_anchor=(1.05, 1), loc='upper left')

# Gráfico para el conjunto de prueba
ax2 = axes[1]
for digit in range(10):
    mask = y_test == digit
    ax2.scatter(Y_test_umap[mask, 0], Y_test_umap[mask, 1],
                c=DIGIT_COLORS[digit],
                label=f'{digit}',
                alpha=0.7, s=25)
ax2.set_title('Proyección UMAP: Conjunto de Prueba (Datos No Vistos)', fontsize=14, fontweight='bold', pad=15)
ax2.set_xlabel('UMAP 1', fontsize=12)
ax2.set_ylabel('UMAP 2', fontsize=12)
ax2.set_aspect('equal')
ax2.grid(True, alpha=0.3)
ax2.legend(title="Dígito", bbox_to_anchor=(1.05, 1), loc='upper left')

plt.tight_layout()
plt.show()
```
